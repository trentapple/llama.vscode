{
  "name": "llama-vscode",
  "displayName": "llama-vscode",
  "description": "Local LLM-assisted text completion using llama.cpp",
  "version": "0.0.12",
  "publisher": "ggml-org",
  "repository": "https://github.com/ggml-org/llama.vscode",
  "engines": {
    "vscode": "^1.70.0"
  },
  "icon": "llama.png",
  "activationEvents": [
    "onLanguage:plaintext",
    "onLanguage:javascript",
    "onLanguage:typescript",
    "onCommand.acceptFirstLine"
  ],
  "main": "./dist/extension.js",
  "contributes": {
    "languages": [
      {
        "id": "plaintext",
        "aliases": [
          "Plain Text"
        ],
        "extensions": [
          ".txt"
        ]
      }
    ],
    "commands": [
      {
        "command": "extension.triggerInlineCompletion",
        "title": "llama-vscode: Trigger Inline Completion"
      },
      {
        "command": "extension.triggerNoCacheCompletion",
        "title": "llama-vscode: Trigger No Cache Completion"
      },
      {
        "command": "extension.copyIntercept",
        "title": "llama-vscode: Copy Intercept"
      },
      {
        "command": "extension.cutIntercept",
        "title": "llama-vscode: Cut Intercept"
      },
      {
        "command": "extension.acceptFirstLine",
        "title": "llama-vscode: Accept First Line"
      },
      {
        "command": "extension.acceptFirstWord",
        "title": "llama-vscode: Accept First Word"
      },
      {
        "command": "extension.copyChunks",
        "title": "llama-vscode: Copy Chunks"
      },
      {
        "command": "extension.showMenu",
        "title": "llama-vscode: Show Menu"
      },
      {
        "command": "extension.askAi",
        "title": "llama-vscode: Ask AI"
      },
      {
        "command": "extension.askAiWithContext",
        "title": "llama-vscode: Ask AI With Context"
      },
      {
        "command": "extension.editSelectedText",
        "title": "llama-vscode: Edit Selected Text with AI"
      },
      {
        "command": "extension.acceptTextEdit",
        "title": "llama-vscode: Accept Text Edit Suggestion"
      },
      {
        "command": "extension.rejectTextEdit",
        "title": "llama-vscode: Reject Text Edit Suggestion"
      },
      {
        "command": "extension.generateGitCommitMessage",
        "title": "llama-vscode: Generate Commit Message",
        "icon": "$(sparkle)"
      }
    ],
    "keybindings": [
      {
        "key": "tab",
        "command": "editor.action.inlineSuggest.commit",
        "when": "inlineSuggestionVisible"
      },
      {
        "command": "extension.triggerInlineCompletion",
        "key": "ctrl+l",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.triggerNoCacheCompletion",
        "key": "ctrl+shift+l",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.copyChunks",
        "key": "ctrl+shift+,",
        "when": "true"
      },
      {
        "command": "extension.copyIntercept",
        "key": "ctrl+c",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.cutIntercept",
        "key": "ctrl+x",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.acceptFirstLine",
        "key": "shift+tab",
        "when": "editorTextFocus && inlineSuggestionVisible"
      },
      {
        "command": "extension.acceptFirstWord",
        "key": "ctrl+right",
        "when": "editorTextFocus && inlineSuggestionVisible"
      },
      {
        "command": "extension.showMenu",
        "key": "ctrl+shift+m",
        "when": "true"
      },
      {
        "command": "extension.askAi",
        "key": "ctrl+;",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.askAiWithContext",
        "key": "ctrl+Shift+;",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.editSelectedText",
        "key": "ctrl+shift+e",
        "when": "editorHasSelection"
      },
      {
        "command": "extension.acceptTextEdit",
        "key": "tab",
        "when": "editorTextFocus && textEditSuggestionVisible"
      },
      {
        "command": "extension.rejectTextEdit",
        "key": "escape",
        "when": "editorTextFocus && textEditSuggestionVisible"
      }
    ],
    "configuration": {
      "type": "object",
      "title": "llama.vscode Configuration",
      "properties": {
        "llama-vscode.launch_completion": {
          "type": "string",
          "default": "cd c:/ai ; ./llama-server.exe -m Qwen2.5-Coder-1.5B.Q8_0.gguf --host 0.0.0.0 --port 8012 -c 2048 -ub 1024 -b 1024 -dt 0.1 --ctx-size 0 --cache-reuse 256",
          "description": "Shell command for starting fim llama.cpp server, executed from the menu"
        },
        "llama-vscode.launch_chat": {
          "type": "string",
          "default": "cd c:/ai ; ./llama-server.exe -m qwen2.5-coder-3b-instruct-q6_k.gguf -ngl 99 --port 8011 --path C:/llama.cpp/llama.cpp/examples/server/webui/dist",
          "description": "Shell command for starting chat llama.cpp server, executed from the menu"
        },
        "llama-vscode.launch_embeddings": {
          "type": "string",
          "default": "cd c:/ai ; ./llama-server.exe -m all-MiniLM-L6-v2-Q8_0.gguf --port 8010",
          "description": "Shell command for starting chat llama.cpp server, executed from the menu"
        },
        "llama-vscode.launch_training_completion": {
          "type": "string",
          "default": "",
          "description": "Shell command for starting training a completion (fim) model from the menu"
        },
        "llama-vscode.launch_training_chat": {
          "type": "string",
          "default": "",
          "description": "Shell command for starting training a chat model from the menu"
        },
        "llama-vscode.lora_completion": {
          "type": "string",
          "default": "",
          "description": "Path to the lora adapter file for the completion model. If not empty it will be used (appends --lora lora_completion) on starting the completion server with launch_completion"
        },
        "llama-vscode.lora_chat": {
          "type": "string",
          "default": "",
          "description": "Path to the lora adapter file for the chat model. If not empty it will be used (appends --lora lora_chat) on starting the completion server with launch_chat"
        },
        "llama-vscode.endpoint": {
          "type": "string",
          "default": "http://127.0.0.1:8012",
          "description": "The URL to be used by the extension for code completion. For Ollama, typically http://127.0.0.1:11434"
        },
        "llama-vscode.endpoint_chat": {
          "type": "string",
          "default": "http://127.0.0.1:8011",
          "description": "The URL to be used by the extension for chat with ai. For Ollama, typically http://127.0.0.1:11434"
        },
        "llama-vscode.endpoint_embeddings": {
          "type": "string",
          "default": "http://127.0.0.1:8010",
          "description": "The URL to be used by the extension for creating embeddings. For Ollama, typically http://127.0.0.1:11434"
        },
        "llama-vscode.auto": {
          "type": "boolean",
          "default": true,
          "description": "If code completion should be trggered automatically (true) or only by pressing Ctrl+l."
        },
        "llama-vscode.api_key": {
          "type": "string",
          "default": "",
          "description": "llama.cpp completion server api key or OpenAI endpoint API key (optional)"
        },
        "llama-vscode.api_key_chat": {
          "type": "string",
          "default": "",
          "description": "llama.cpp chat server api key"
        },
        "llama-vscode.api_key_embeddings": {
          "type": "string",
          "default": "",
          "description": "llama.cpp embeddings server api key"
        },
        "llama-vscode.self_signed_certificate": {
          "type": "string",
          "default": "",
          "description": "self-signed certificate file - path/to/cert.pem"
        },
        "llama-vscode.n_prefix": {
          "type": "number",
          "default": 256,
          "description": "number of lines before the cursor location to include in the local prefix"
        },
        "llama-vscode.n_suffix": {
          "type": "number",
          "default": 64,
          "description": "number of lines after  the cursor location to include in the local suffix"
        },
        "llama-vscode.n_predict": {
          "type": "number",
          "default": 128,
          "description": "max number of tokens to predict"
        },
        "llama-vscode.t_max_prompt_ms": {
          "type": "number",
          "default": 500,
          "description": "max alloted time for the prompt processing (TODO: not yet supported)"
        },
        "llama-vscode.t_max_predict_ms": {
          "type": "number",
          "default": 500,
          "description": "max alloted time for the prediction"
        },
        "llama-vscode.show_info": {
          "type": "boolean",
          "default": true,
          "description": "show extra info about the inference (false - disabled, true - show extra info in status line)"
        },
        "llama-vscode.max_line_suffix": {
          "type": "number",
          "default": 8,
          "description": "do not auto-trigger FIM completion if there are more than this number of characters to the right of the cursor"
        },
        "llama-vscode.max_cache_keys": {
          "type": "number",
          "default": 250,
          "description": "max number of cached completions to keep in result_cache"
        },
        "llama-vscode.ring_n_chunks": {
          "type": "number",
          "default": 16,
          "description": "max number of chunks to pass as extra context to the server (0 to disable)"
        },
        "llama-vscode.ring_chunk_size": {
          "type": "number",
          "default": 64,
          "description": "max size of the chunks (in number of lines). Note: adjust these numbers so that you don't overrun your context at ring_n_chunks = 64 and ring_chunk_size = 64 you need ~32k context"
        },
        "llama-vscode.ring_scope": {
          "type": "number",
          "default": 1024,
          "description": "the range around the cursor position (in number of lines) for gathering chunks after FIM"
        },
        "llama-vscode.ring_update_ms": {
          "type": "number",
          "default": 1000,
          "description": "how often to process queued chunks in normal mode"
        },
        "llama-vscode.rag_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable rag features - i.e. chat with AI with project context"
        },
        "llama-vscode.rag_chunk_max_chars": {
          "type": "number",
          "default": 2000,
          "description": "Max number of chars per RAG chunk"
        },
        "llama-vscode.rag_max_lines_per_chunk": {
          "type": "number",
          "default": 60,
          "description": "Max number of lines per RAG chunk"
        },
        "llama-vscode.rag_max_chars_per_chunk_line": {
          "type": "number",
          "default": 300,
          "description": "max chars for a chunk line, the rest of the line is cut"
        },
        "llama-vscode.rag_max_files": {
          "type": "number",
          "default": 10000,
          "description": "max files to index for RAG search, 0 to switch off indexing"
        },
        "llama-vscode.rag_max_chunks": {
          "type": "number",
          "default": 30000,
          "description": "max cunks for the RAG search"
        },
        "llama-vscode.rag_max_bm25_filter_chunks": {
          "type": "number",
          "default": 47,
          "description": "max RAG chunks to filter with BM25 algorithm"
        },
        "llama-vscode.rag_max_embedding_filter_chunks": {
          "type": "number",
          "default": 5,
          "description": "max RAG chunks to provide as context to the LLM"
        },
        "llama-vscode.rag_max_context_files": {
          "type": "number",
          "default": 3,
          "description": "max number of complete files to send as context to the LLM"
        },
        "llama-vscode.rag_max_context_file_chars": {
          "type": "number",
          "default": 5000,
          "description": "max chars for a context file. If the file is bigger it will be cut to avoid too big context."
        },
        "llama-vscode.language": {
          "type": "string",
          "default": "en",
          "description": "language: bg - Bulgarian (Български), cn - Chinese (中文), en - English, fr - French (Français), de - German (Deutsch), ru - Russian (Русский), es - Spanish (Español)"
        },
        "llama-vscode.enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable completions"
        },
        "llama-vscode.languageSettings": {
          "type": "object",
          "default": {
            "*": true
          },
          "additionalProperties": {
            "type": "boolean"
          },
          "description": "Enable/disable suggestions for specific languages"
        },
        "llama-vscode.use_openai_endpoint": {
          "type": "boolean",
          "default": false,
          "description": "[EXPERIMENTAL] Use OAI endpoint. Slow and poor quality - avoid using"
        },
        "llama-vscode.openai_client_model": {
          "type": "string",
          "default": "",
          "description": "The FIM friendly model supported by your OpenAI compatible endpoint to be used (e.g., Qwen2.5-Coder-14B-4-bit)"
        },
        "llama-vscode.openai_prompt_template": {
          "type": "string",
          "default": "<|fim_prefix|>{inputPrefix}{prompt}<|fim_suffix|>{inputSuffix}<|fim_middle|>",
          "description": "The prompt template to be used for the OpenAI compatible endpoint."
        },
        "llama-vscode.completion_model": {
          "type": "string",
          "default": "",
          "description": "Model name for code completion (FIM). Leave empty to use server default. For Ollama, use model names like 'codellama:7b' or 'deepseek-coder:6.7b'."
        },
        "llama-vscode.chat_model": {
          "type": "string",
          "default": "",
          "description": "Model name for chat interactions. Leave empty to use server default. For Ollama, use model names like 'llama3.1:8b' or 'qwen2.5-coder:7b'."
        },
        "llama-vscode.embeddings_model": {
          "type": "string",
          "default": "",
          "description": "Model name for embeddings. Leave empty to use server default. For Ollama, use model names like 'nomic-embed-text' or 'all-minilm'."
        },
        "llama-vscode.use_ollama": {
          "type": "boolean",
          "default": false,
          "description": "Enable Ollama compatibility mode. When enabled, the extension will use Ollama-specific API endpoints and model handling."
        }
      }
    },
    "menus": {
      "scm/title": [
        {
          "command": "extension.generateGitCommitMessage",
          "when": "scmProvider == git",
          "group": "navigation"
        }
      ],
      "editor/context": [
        {
          "command": "extension.editSelectedText",
          "when": "editorHasSelection",
          "group": "llama@1"
        }
      ]
    }
  },
  "scripts": {
    "watch": "tsc -watch -p ./"
  },
  "dependencies": {
    "axios": "^1.1.2",
    "ignore": "^7.0.4",
    "openai": "^4.80.1"
  },
  "devDependencies": {
    "@types/jest": "^29.5.14",
    "@types/node": "^18.0.0",
    "@types/vscode": "^1.70.0",
    "typescript": "^4.8.0",
    "webpack": "^5.0.0",
    "webpack-cli": "^4.0.0"
  },
  "extensionDependencies": [
    "vscode.git"
  ]
}
