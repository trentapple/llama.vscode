{
  "name": "llama-vscode",
  "displayName": "llama-vscode",
  "description": "Local LLM-assisted text completion using llama.cpp",
  "version": "0.0.12",
  "publisher": "ggml-org",
  "repository": "https://github.com/ggml-org/llama.vscode",
  "engines": {
    "vscode": "^1.100.0"
  },
  "icon": "llama.png",
  "activationEvents": [
    "onLanguage:plaintext",
    "onLanguage:javascript",
    "onLanguage:typescript",
    "onCommand.acceptFirstLine"
  ],
  "main": "./dist/extension.js",
  "contributes": {
    "languages": [
      {
        "id": "plaintext",
        "aliases": [
          "Plain Text"
        ],
        "extensions": [
          ".txt"
        ]
      }
    ],
    "commands": [
      {
        "command": "extension.triggerInlineCompletion",
        "title": "llama-vscode: Trigger Inline Completion"
      },
      {
        "command": "extension.triggerNoCacheCompletion",
        "title": "llama-vscode: Trigger No Cache Completion"
      },
      {
        "command": "extension.copyIntercept",
        "title": "llama-vscode: Copy Intercept"
      },
      {
        "command": "extension.cutIntercept",
        "title": "llama-vscode: Cut Intercept"
      },
      {
        "command": "extension.acceptFirstLine",
        "title": "llama-vscode: Accept First Line"
      },
      {
        "command": "extension.acceptFirstWord",
        "title": "llama-vscode: Accept First Word"
      },
      {
        "command": "extension.copyChunks",
        "title": "llama-vscode: Copy Chunks"
      },
      {
        "command": "extension.showMenu",
        "title": "llama-vscode: Show Menu"
      },
      {
        "command": "extension.askAi",
        "title": "llama-vscode: Ask AI"
      },
      {
        "command": "extension.askAiWithContext",
        "title": "llama-vscode: Ask AI With Context"
      },
      {
        "command": "extension.editSelectedText",
        "title": "llama-vscode: Edit Selected Text with AI"
      },
      {
        "command": "extension.acceptTextEdit",
        "title": "llama-vscode: Accept Text Edit Suggestion"
      },
      {
        "command": "extension.rejectTextEdit",
        "title": "llama-vscode: Reject Text Edit Suggestion"
      },
      {
        "command": "extension.killAgent",
        "title": "llama-vscode: Kill Agent Session"
      },
      {
        "command": "extension.generateGitCommitMessage",
        "title": "llama-vscode: Generate Commit Message",
        "icon": "$(sparkle)"
      },
      {
        "command": "extension.showLlamaWebview",
        "title": "llama-vscode: Show Llama Agent",
        "icon": "$(window)"
      }
    ],
    "views": {
      "explorer": [
        {
          "id": "llama-vscode.webview",
          "name": "Llama Agent",
          "type": "webview"
        }
      ]
    },
    "keybindings": [
      {
        "key": "tab",
        "command": "editor.action.inlineSuggest.commit",
        "when": "inlineSuggestionVisible"
      },
      {
        "command": "extension.triggerInlineCompletion",
        "key": "ctrl+l",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.triggerNoCacheCompletion",
        "key": "ctrl+shift+l",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.copyChunks",
        "key": "ctrl+shift+,",
        "when": "true"
      },
      {
        "command": "extension.copyIntercept",
        "key": "ctrl+c",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.cutIntercept",
        "key": "ctrl+x",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.acceptFirstLine",
        "key": "shift+tab",
        "when": "editorTextFocus && inlineSuggestionVisible"
      },
      {
        "command": "extension.acceptFirstWord",
        "key": "ctrl+right",
        "when": "editorTextFocus && inlineSuggestionVisible"
      },
      {
        "command": "extension.showMenu",
        "key": "ctrl+shift+m",
        "when": "true"
      },
      {
        "command": "extension.showLlamaWebview",
        "key": "ctrl+shift+a",
        "when": "true"
      },
      {
        "command": "extension.askAi",
        "key": "ctrl+;",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.askAiWithContext",
        "key": "ctrl+Shift+;",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.askAiWithTools",
        "key": "ctrl+Shift+t",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.editSelectedText",
        "key": "ctrl+shift+e",
        "when": "editorHasSelection"
      },
      {
        "command": "extension.acceptTextEdit",
        "key": "tab",
        "when": "editorTextFocus && textEditSuggestionVisible"
      },
      {
        "command": "extension.killAgent",
        "key": "ctrl+k ctrl+a",
        "when": "editorTextFocus"
      },
      {
        "command": "extension.rejectTextEdit",
        "key": "escape",
        "when": "editorTextFocus && textEditSuggestionVisible"
      }
    ],
    "configuration": {
      "type": "object",
      "title": "llama.vscode Configuration",
      "properties": {
        "llama-vscode.launch_completion": {
          "type": "string",
          "default": "cd c:/ai ; ./llama-server.exe -m Qwen2.5-Coder-1.5B.Q8_0.gguf --host 0.0.0.0 --port 8012 -c 2048 -ub 1024 -b 1024 -dt 0.1 --ctx-size 0 --cache-reuse 256",
          "description": "Shell command for starting fim llama.cpp server, executed from the menu"
        },
        "llama-vscode.launch_chat": {
          "type": "string",
          "default": "cd c:/ai ; ./llama-server.exe -m qwen2.5-coder-3b-instruct-q6_k.gguf -ngl 99 --port 8011 --path C:/llama.cpp/llama.cpp/examples/server/webui/dist",
          "description": "Shell command for starting chat llama.cpp server, executed from the menu"
        },
        "llama-vscode.launch_embeddings": {
          "type": "string",
          "default": "cd c:/ai ; ./llama-server.exe -m all-MiniLM-L6-v2-Q8_0.gguf --port 8010",
          "description": "Shell command for starting chat llama.cpp server, executed from the menu"
        },
        "llama-vscode.launch_training_completion": {
          "type": "string",
          "default": "",
          "description": "Shell command for starting training a completion (fim) model from the menu"
        },
        "llama-vscode.launch_training_chat": {
          "type": "string",
          "default": "",
          "description": "Shell command for starting training a chat model from the menu"
        },
        "llama-vscode.lora_completion": {
          "type": "string",
          "default": "",
          "description": "Path to the lora adapter file for the completion model. If not empty it will be used (appends --lora lora_completion) on starting the completion server with launch_completion"
        },
        "llama-vscode.lora_chat": {
          "type": "string",
          "default": "",
          "description": "Path to the lora adapter file for the chat model. If not empty it will be used (appends --lora lora_chat) on starting the completion server with launch_chat"
        },
        "llama-vscode.endpoint": {
          "type": "string",
          "default": "http://127.0.0.1:8012",
          "description": "The URL to be used by the extension for code completion."
        },
        "llama-vscode.endpoint_chat": {
          "type": "string",
          "default": "http://127.0.0.1:8011",
          "description": "The URL to be used by the extension for chat with ai."
        },
        "llama-vscode.endpoint_tools": {
          "type": "string",
          "default": "http://127.0.0.1:8011",
          "description": "The URL to be used by the extension for chat with ai with tools use."
        },
        "llama-vscode.endpoint_embeddings": {
          "type": "string",
          "default": "http://127.0.0.1:8010",
          "description": "The URL to be used by the extension for creating embeddings."
        },
        "llama-vscode.ai_api_version": {
          "type": "string",
          "default": "v1",
          "description": "The version of the API of the model. It is appended to the endpoints for chat and tools"
        },
        "llama-vscode.ai_model": {
          "type": "string",
          "default": "google/gemini-2.5-flash",
          "description": "The model name. This is used in the request to the API. It is important when OpenRouter is used (for example google/gemini-2.5-flash)."
        },
        "llama-vscode.auto": {
          "type": "boolean",
          "default": true,
          "description": "If code completion should be trggered automatically (true) or only by pressing Ctrl+l."
        },
        "llama-vscode.api_key": {
          "type": "string",
          "default": "",
          "description": "llama.cpp completion server api key or OpenAI endpoint API key (optional)"
        },
        "llama-vscode.api_key_chat": {
          "type": "string",
          "default": "",
          "description": "llama.cpp chat server api key"
        },
        "llama-vscode.api_key_tools": {
          "type": "string",
          "default": "",
          "description": "llama.cpp AI with tools server api key"
        },
        "llama-vscode.api_key_embeddings": {
          "type": "string",
          "default": "",
          "description": "llama.cpp embeddings server api key"
        },
        "llama-vscode.self_signed_certificate": {
          "type": "string",
          "default": "",
          "description": "self-signed certificate file - path/to/cert.pem"
        },
        "llama-vscode.n_prefix": {
          "type": "number",
          "default": 256,
          "description": "number of lines before the cursor location to include in the local prefix"
        },
        "llama-vscode.n_suffix": {
          "type": "number",
          "default": 64,
          "description": "number of lines after  the cursor location to include in the local suffix"
        },
        "llama-vscode.n_predict": {
          "type": "number",
          "default": 128,
          "description": "max number of tokens to predict"
        },
        "llama-vscode.t_max_prompt_ms": {
          "type": "number",
          "default": 500,
          "description": "max alloted time for the prompt processing (TODO: not yet supported)"
        },
        "llama-vscode.t_max_predict_ms": {
          "type": "number",
          "default": 500,
          "description": "max alloted time for the prediction"
        },
        "llama-vscode.show_info": {
          "type": "boolean",
          "default": true,
          "description": "show extra info about the inference (false - disabled, true - show extra info in status line)"
        },
        "llama-vscode.max_line_suffix": {
          "type": "number",
          "default": 8,
          "description": "do not auto-trigger FIM completion if there are more than this number of characters to the right of the cursor"
        },
        "llama-vscode.max_cache_keys": {
          "type": "number",
          "default": 250,
          "description": "max number of cached completions to keep in result_cache"
        },
        "llama-vscode.ring_n_chunks": {
          "type": "number",
          "default": 16,
          "description": "max number of chunks to pass as extra context to the server (0 to disable)"
        },
        "llama-vscode.ring_chunk_size": {
          "type": "number",
          "default": 64,
          "description": "max size of the chunks (in number of lines). Note: adjust these numbers so that you don't overrun your context at ring_n_chunks = 64 and ring_chunk_size = 64 you need ~32k context"
        },
        "llama-vscode.ring_scope": {
          "type": "number",
          "default": 1024,
          "description": "the range around the cursor position (in number of lines) for gathering chunks after FIM"
        },
        "llama-vscode.ring_update_ms": {
          "type": "number",
          "default": 1000,
          "description": "how often to process queued chunks in normal mode"
        },
        "llama-vscode.rag_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable rag features - i.e. chat with AI with project context"
        },
        "llama-vscode.rag_chunk_max_chars": {
          "type": "number",
          "default": 2000,
          "description": "Max number of chars per RAG chunk"
        },
        "llama-vscode.rag_max_lines_per_chunk": {
          "type": "number",
          "default": 60,
          "description": "Max number of lines per RAG chunk"
        },
        "llama-vscode.rag_max_chars_per_chunk_line": {
          "type": "number",
          "default": 300,
          "description": "max chars for a chunk line, the rest of the line is cut"
        },
        "llama-vscode.rag_max_files": {
          "type": "number",
          "default": 10000,
          "description": "max files to index for RAG search, 0 to switch off indexing"
        },
        "llama-vscode.rag_max_chunks": {
          "type": "number",
          "default": 30000,
          "description": "max cunks for the RAG search"
        },
        "llama-vscode.rag_max_bm25_filter_chunks": {
          "type": "number",
          "default": 47,
          "description": "max RAG chunks to filter with BM25 algorithm"
        },
        "llama-vscode.rag_max_embedding_filter_chunks": {
          "type": "number",
          "default": 5,
          "description": "max RAG chunks to provide as context to the LLM"
        },
        "llama-vscode.rag_max_context_files": {
          "type": "number",
          "default": 3,
          "description": "max number of complete files to send as context to the LLM"
        },
        "llama-vscode.rag_max_context_file_chars": {
          "type": "number",
          "default": 5000,
          "description": "max chars for a context file. If the file is bigger it will be cut to avoid too big context."
        },
        "llama-vscode.tool_run_terminal_command_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool run_terminal_command"
        },
        "llama-vscode.tool_permit_some_terminal_commands": {
          "type": "boolean",
          "default": false,
          "description": "Permit AI to execute some safe terminal commands, which do not change the environment (no guarantee) "
        },
        "llama-vscode.tool_permit_file_changes": {
          "type": "boolean",
          "default": false,
          "description": "Permit AI to edit and delete files without user confirmation"
        },
        "llama-vscode.tool_search_source_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool search_source"
        },
        "llama-vscode.tool_read_file_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool read_file"
        },
        "llama-vscode.tool_list_directory_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool list_directory"
        },
        "llama-vscode.tool_regex_search_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool regex_search"
        },
        "llama-vscode.tool_delete_file_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool delete_file"
        },
        "llama-vscode.tool_get_diff_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool delete_file"
        },
        "llama-vscode.tool_edit_file_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool edit_file"
        },
        "llama-vscode.tool_ask_user_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable tool ask_user"
        },
        "llama-vscode.tool_custom_tool_enabled": {
          "type": "boolean",
          "default": false,
          "description": "Enable/disable tool custom_tool"
        },
        "llama-vscode.tool_custom_tool_description": {
          "type": "string",
          "default": "Use this tool to get information about ...",
          "description": "Description for the custom_tool, which will be sent to the AI. The result of the tool execution will be the content of file or web page from property custom_tool_source."
        },
        "llama-vscode.tool_custom_tool_source": {
          "type": "string",
          "default": "https://news.smol.ai/",
          "description": "The long name of a text file (for example c:\\ai\\llms_basics.txt) or URL of a web page (should start with 'http' i.e. https://news.test.com), which content will be returned by the custom_tool when called. Not all web pages are parsed correctly. "
        },
        "llama-vscode.tool_custom_eval_tool_enabled": {
          "type": "boolean",
          "default": false,
          "description": "Enable/disable tool custom_eval_tool"
        },
        "llama-vscode.tool_custom_eval_tool_description": {
          "type": "string",
          "default": "Use this tool to calculate an arithmetic expression.Example: '15 + 4' or '12/4'",
          "description": "Description for the custom_eval_tool, which will be sent to the AI. The result of the tool will be the result from the execution of the typescript code from setting custom_eval_tool_code. This is powerful, but could be security risk. Be careful."
        },
        "llama-vscode.tool_custom_eval_tool_property_description": {
          "type": "string",
          "default": "The arithmetic expression to be calculated. Example: '5 + 7' or '(3456*5678) - 256' ",
          "description": "The description of the property (input) for the custom_eval_tool. "
        },
        "llama-vscode.tool_custom_eval_tool_code": {
          "type": "string",
          "default": "function(input) { return eval(input); }",
          "description": "The typescript function to be executed when the tool is called. It should have one parameter of type string. When called, the parameter will be the value provided by the AI in the tool property. This is powerful, but could be security risk. Be careful."
        },       
        "llama-vscode.tools_max_iterations": {
          "type": "number",
          "default": 20,
          "description": "Max number of iterations with AI when working with tools. If you are working with paid AI providers, big number here could result in higher costs."
        },
        "llama-vscode.tools_log_calls": {
          "type": "boolean",
          "default": false,
          "description": "Show the details about the tools calls in UI - arguments and results."
        },
        "llama-vscode.language": {
          "type": "string",
          "default": "en",
          "description": "language: bg - Bulgarian (Български), cn - Chinese (中文), en - English, fr - French (Français), de - German (Deutsch), ru - Russian (Русский), es - Spanish (Español)"
        },
        "llama-vscode.enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable/disable completions"
        },
        "llama-vscode.languageSettings": {
          "type": "object",
          "default": {
            "*": true
          },
          "additionalProperties": {
            "type": "boolean"
          },
          "description": "Enable/disable suggestions for specific languages"
        },
        "llama-vscode.use_openai_endpoint": {
          "type": "boolean",
          "default": false,
          "description": "[EXPERIMENTAL] Use OAI endpoint. Slow and poor quality - avoid using"
        },
        "llama-vscode.openai_client_model": {
          "type": "string",
          "default": "",
          "description": "The FIM friendly model supported by your OpenAI compatible endpoint to be used (e.g., Qwen2.5-Coder-14B-4-bit)"
        },
        "llama-vscode.openai_prompt_template": {
          "type": "string",
          "default": "<|fim_prefix|>{inputPrefix}{prompt}<|fim_suffix|>{inputSuffix}<|fim_middle|>",
          "description": "The prompt template to be used for the OpenAI compatible endpoint."
        }
      }
    },
    "menus": {
      "scm/title": [
        {
          "command": "extension.generateGitCommitMessage",
          "when": "scmProvider == git",
          "group": "navigation"
        }
      ],
      "editor/context": [
        {
          "command": "extension.editSelectedText",
          "when": "editorHasSelection",
          "group": "llama@1"
        },
        {
          "command": "extension.showLlamaWebview",
          "when": "editorHasSelection",
          "group": "llama@1"
        }
      ]
    }
  },
  "scripts": {
    "watch": "tsc -watch -p ./",
    "build-ui": "cd ui && npm install && npm run build",
    "dev-ui": "cd ui && npm install && npm run dev",
    "postinstall": "npm run build-ui"
  },
  "dependencies": {
    "axios": "^1.1.2",
    "globby": "^14.1.0",
    "ignore": "^7.0.4",
    "openai": "^4.80.1",
    "picomatch": "^4.0.2",
    "simple-git": "^3.28.0",
    "xmldom": "^0.6.0"
  },
  "devDependencies": {
    "@types/jest": "^29.5.14",
    "@types/micromatch": "^4.0.9",
    "@types/node": "^18.0.0",
    "@types/picomatch": "^4.0.0",
    "@types/vscode": "^1.100.0",
    "@types/xmldom": "^0.1.34",
    "typescript": "^4.8.0",
    "webpack": "^5.100.2",
    "webpack-cli": "^4.10.0"
  },
  "extensionDependencies": [
    "vscode.git"
  ]
}
